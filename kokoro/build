#!/usr/bin/env python3
# Copyright 2019 The ChromiumOS Authors
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Do all the things!"""

import fnmatch
import glob
import logging
import os
from pathlib import Path
import re
import shutil
import subprocess
import sys

import kokoro
import libdot  # pylint: disable=wrong-import-order


# Where kokoro expects files to end up.  Anything put in this dir will be
# uploaded & archived, while everything else will be discarded.
ARTIFACTS_DIR = kokoro.LIBAPPS_DIR / "artifacts"

# When doing official internal builds, some files might be provided ahead of
# time via this path.
INPUT_ARTIFACTS_DIR = os.path.join(kokoro.LIBAPPS_DIR, "google3-inputs")

# Where we drop all our comment files for passing back to gerrit.
# Kokoro can handle more than one file, but they must all be named the same
# "gerrit_comments.json".  So we have to namespace the files under dirs.
# We'll use a convention like gerrit_comments/$PKG/$TOOL/.
GERRIT_COMMENTS_NAME = "gerrit_comments.json"
GERRIT_COMMENTS_DIR = os.path.join(ARTIFACTS_DIR, "gerrit_comments")

# Where we drop the test results for ingestion into sponge.  Since it only
# handles XML junit/xunit results, we need to format+output manually.
# Kokoro can handle more than one file, but they must all be named the same
# "sponge_log.log" & "sponge_log.xml" (we'll ignore retry support as we don't
# use it).  So we have to namespace the files under dirs.
# We'll use a convention like sponge/$PKG/$TOOL/.
# NB: sponge_log.log is only loaded if sponge_log.xml is found.
SPONGE_LOG_NAME = "sponge_log.log"
SPONGE_XML_NAME = "sponge_log.xml"
SPONGE_RESULTS_DIR = os.path.join(ARTIFACTS_DIR, "sponge")

# Where to get depot_tools.
DEPOT_TOOLS_URI = "https://chromium.googlesource.com/chromium/tools/depot_tools"
DEPOT_TOOLS_DIR = kokoro.LIBAPPS_DIR / "depot_tools"


def is_lint_test_only() -> bool:
    """Whether we're only linting+testing the current tree."""
    # This env var is passed down by our script.
    return bool(os.environ.get("LIBAPPS_LINT_TEST"))


def is_presubmit():
    """Whether we're testing a pending CL."""
    # This env var is passed down by kokoro.
    return bool(os.environ.get("KOKORO_GERRIT_REVISION"))


def should_build_plugin():
    """Whether we should build the ssh_client plugin code.

    Building this native code with the nacl toolchain dominates our build times
    (usually ~75% of the entire build).  Lets skip it if we don't think any of
    changes are relevant.
    """
    logging.info("Detecting whether to build plugin")

    if is_lint_test_only():
        logging.info("Only running lint+unittest")
        return False

    # Release builders always build the plugin.
    if not is_presubmit():
        logging.info("Release builder: always building the plugin!")
        return True

    check_paths = (
        # The CI itself.
        "kokoro/*",
        # Utility libraries.
        "libdot/bin/*",
        # The plugin code itself.
        "ssh_client/*",
    )
    result = libdot.run(
        [
            "git",
            "show",
            "--format=",
            "--name-only",
            os.environ["KOKORO_GERRIT_REVISION"],
        ],
        capture_output=True,
        cwd=kokoro.LIBAPPS_DIR,
    )
    relevant_paths = list(
        x
        for x in result.stdout.decode("utf-8").splitlines()
        if any(fnmatch.fnmatch(x, check) for check in check_paths)
    )
    if relevant_paths:
        logging.info(
            "Presubmit: some files might affect the plugin:\n%s",
            "\n".join(relevant_paths),
        )
    else:
        logging.info("Presubmit: no files affect the plugin; skipping")
    return bool(relevant_paths)


def build_archive():
    """Move compiled files to artifacts output tree."""
    # Unpack the nassh extensions so they can be packaged & signed.
    nassh_dir = os.path.join(kokoro.LIBAPPS_DIR, "nassh")
    dist_dir = os.path.join(nassh_dir, "dist")
    for archive in glob.glob(os.path.join(dist_dir, "*.zip")):
        # The archives look like:
        #   SecureShellApp-0.20.zip
        #   SecureShellApp-dev-0.20.7220.2157.zip
        # We want to output dirs like:
        #   SecureShellApp/
        #   SecureShellApp-dev/
        output = os.path.basename(archive).rsplit("-", 1)[0]
        libdot.run(
            ["unzip", archive, "-d", os.path.join(ARTIFACTS_DIR, output)]
        )

    # Copy over the nacl ssh build with debug info.
    if should_build_plugin():
        ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, "ssh_client")
        os.rename(
            os.path.join(ssh_client_dir, "output", "release.tar.xz"),
            os.path.join(ARTIFACTS_DIR, "ssh_client-release.tar.xz"),
        )


def build():
    """Build the various components."""
    if should_build_plugin():
        # Build the ssh client code.
        ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, "ssh_client")
        cmd = ["./build.sh"]
        if not is_presubmit():
            cmd += ["--official-release"]
        libdot.run(cmd, cwd=ssh_client_dir)
        # Copy output files.
        libdot.run(
            ["cp", "-r", "./ssh_client/output/plugin/", "./nassh/"],
            cwd=kokoro.LIBAPPS_DIR,
        )

    nassh_dir = os.path.join(kokoro.LIBAPPS_DIR, "nassh")

    # If updated translations exist, use them.
    tot_l10n = os.path.join(INPUT_ARTIFACTS_DIR, "translations")
    if os.path.exists(tot_l10n):
        libdot.run(
            [
                "./bin/import-translations",
                "--skip-git",
                os.path.join(tot_l10n, "messages_fs", "_locales"),
            ],
            cwd=nassh_dir,
        )

    # Build the nassh program.
    dist_dir = os.path.join(nassh_dir, "dist")
    shutil.rmtree(dist_dir, ignore_errors=True)
    libdot.run(["./bin/mkdist"], cwd=nassh_dir)

    build_archive()


def _run_linter(cmd, cwd):
    """Run the linter taking into account presubmit bot state."""
    result = libdot.run(cmd, check=False, cwd=cwd)
    if result.returncode:
        msg = f"{cmd[0]} linter exited {result.returncode}"
        if is_presubmit():
            logging.error(msg)
            return False
        else:
            logging.warning("%s; ignoring for release builds", msg)
    return True


def test_lint():
    """Lint all our files."""
    # Log versions of linter tools since we pull some from the container.
    libdot.run([kokoro.DIR / "lint", "--version"])

    ret = []
    for pkg in (
        "kokoro",
        "libdot",
        "hterm",
        "nassh",
        "ssh_client",
        "terminal",
        "wasi-js-bindings",
        "wassh",
    ):
        pkg_dir = kokoro.LIBAPPS_DIR / pkg
        linter = pkg_dir / "lint"
        if not linter.exists():
            linter = pkg_dir / "bin" / "lint"
        cmd = [
            linter,
            "--gerrit-comments-file",
            os.path.join(
                GERRIT_COMMENTS_DIR, pkg, "%(tool)", GERRIT_COMMENTS_NAME
            ),
        ]
        if not _run_linter(cmd, pkg_dir):
            ret.append(("lint", pkg))
    return ret


def test_unittests():
    """Run all the unittests."""
    ret = []
    test_args = ["--no-sandbox", "--reporter", "xunit"]
    for pkg in ("libdot", "hterm", "nassh", "terminal", "wasi-js-bindings"):
        pkg_dir = os.path.join(kokoro.LIBAPPS_DIR, pkg)
        if os.path.exists(os.path.join(pkg_dir, "package.json")):
            cmd = ["test", "--"] + test_args
            run = libdot.npm.run
        else:
            cmd = ["./bin/load_tests"] + test_args
            run = libdot.run
        result = run(
            cmd,
            check=False,
            cwd=pkg_dir,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )

        if result.returncode:
            logging.error("%s test suite exited %i", pkg, result.returncode)
            ret.append(("unittests", pkg))

        # Dump the results to log files for sponge to ingest.
        sponge_dir = os.path.join(SPONGE_RESULTS_DIR, pkg, "unittests")
        os.makedirs(sponge_dir)
        log_file = os.path.join(sponge_dir, SPONGE_LOG_NAME)
        with open(log_file, "wb") as fp:
            fp.write(result.stdout)

        # The test helper will output stuff before/after the XML file, so we
        # need to scrape it out by hand.
        m = re.search(
            rb"^(.*)(<testsuite[ >].*</testsuite>)(.*)$",
            result.stdout,
            flags=re.DOTALL,
        )
        if m:
            log_header, xml_data, log_footer = m.groups()

            xml_file = os.path.join(sponge_dir, SPONGE_XML_NAME)
            with open(xml_file, "wb") as fp:
                fp.write(xml_data)

            print(log_header.decode("utf-8"))
            print(f"<xml data is in {xml_file}>")
            print(log_footer.decode("utf-8"))
        else:
            logging.error(
                "Unable to parse xml results from log output!\n%s",
                result.stdout.decode("utf-8"),
            )
            ret.append(("unittests", pkg))

    return ret


def test():
    """Test the various components."""
    # Run all the tests before aborting.
    failures = test_lint() + test_unittests()
    if failures:
        logging.error("Some tests failed!  See above for details.")
        for step, pkg in failures:
            logging.error("  %s failed %s phase.", pkg, step)
        sys.exit(1)


def _chown(path: Path, uid: int, gid: int) -> None:
    """Helper to chown |path| to |uid| & |gid|."""
    if path.stat().st_uid == uid:
        return

    try:
        os.chown(path, uid, gid)
    except OSError as e:
        logging.warning("Unable to chown(%s, %s, %s): %s", path, uid, gid, e)


def chown_recursive(path: Path, uid: int, gid: int) -> None:
    """Chown all paths under |path| to |uid| & |gid|."""
    _chown(path, uid, gid)
    for p in path.iterdir():
        if p.is_symlink():
            continue
        elif p.is_dir():
            chown_recursive(p, uid, gid)
        elif p.is_file():
            _chown(p, uid, gid)


def clean():
    """Clean up various compiled objects.

    When kokoro is done (pass or fail), it archives the tree via rsync.
    Remove all the compiled objects since we don't need them.
    """
    ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, "ssh_client")
    shutil.rmtree(os.path.join(ssh_client_dir, "output"), ignore_errors=True)

    shutil.rmtree(libdot.node.NODE_MODULES_DIR, ignore_errors=True)

    # Sync uid/gid settings in the output artifacts.  The docker container
    # runs all code as root, but the accounts outside won't be that.  Look
    # at the source tree created outside the container for the right values.
    st = os.stat(kokoro.LIBAPPS_DIR)
    uid = st.st_uid
    gid = st.st_gid
    if uid != os.getuid():
        for path in (ARTIFACTS_DIR, DEPOT_TOOLS_DIR):
            logging.info("Changing ownership of %s to %i:%i", path, uid, gid)
            chown_recursive(path, uid, gid)


def setup():
    """Set up the overall state before we build/test."""
    # Log some system state so we can see current CI/container state.
    libdot.run(["uname", "-a"])
    libdot.run(["cat", "/etc/os-release"], check=False)
    libdot.run(["env"])

    # Initialize the artifacts output.
    logging.info("Setting up artifacts output %s", ARTIFACTS_DIR)
    shutil.rmtree(ARTIFACTS_DIR, ignore_errors=True)
    os.makedirs(ARTIFACTS_DIR)

    # Create the dropbox for gerrit comments.
    os.makedirs(GERRIT_COMMENTS_DIR)
    # Always create a stub comments file.  The kokoro docs don't say this is
    # required, but other users report it doesn't post updates back to Gerrit
    # unless it exists, and we're seeing that same behavior.
    # http://g/kokoro-users/coBW2tNu46g/W8PozUUpAQAJ
    stub = os.path.join(GERRIT_COMMENTS_DIR, GERRIT_COMMENTS_NAME)
    with open(stub, "wb") as fp:
        fp.write(b"[]")

    # Make sure depot_tools is available for vpython.
    if not DEPOT_TOOLS_DIR.exists():
        subprocess.run(
            ["git", "clone", "--depth=1", DEPOT_TOOLS_URI, DEPOT_TOOLS_DIR],
            check=True,
        )
    os.environ["PATH"] = f"{DEPOT_TOOLS_DIR}{os.pathsep}" + os.environ["PATH"]


def get_parser():
    """Get a command line parser."""
    parser = libdot.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--skip-clean",
        dest="clean",
        action="store_false",
        default=True,
        help="Clean up compiled object dirs.",
    )
    return parser


def main(argv):
    """The main func!"""
    parser = get_parser()
    opts = parser.parse_args(argv)
    libdot.node_and_npm_setup()

    setup()
    try:
        build()
        test()
        logging.info("All builds & tests passed!")
    finally:
        if opts.clean:
            clean()


if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
