#!/usr/bin/env python3
# Copyright 2019 The Chromium OS Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Do all the things!"""

import fnmatch
import glob
import logging
import os
import re
import shutil
import subprocess
import sys

import kokoro
import libdot


# Where kokoro expects files to end up.  Anything put in this dir will be
# uploaded & archived, while everything else will be discarded.
ARTIFACTS_DIR = os.path.join(kokoro.LIBAPPS_DIR, 'artifacts')

# When doing official internal builds, some files might be provided ahead of
# time via this path.
INPUT_ARTIFACTS_DIR = os.path.join(kokoro.LIBAPPS_DIR, 'google3-inputs')

# Where we drop all our comment files for passing back to gerrit.
# Kokoro can handle more than one file, but they must all be named the same
# "gerrit_comments.json".  So we have to namespace the files under dirs.
# We'll use a convention like gerrit_comments/$PKG/$TOOL/.
GERRIT_COMMENTS_NAME = 'gerrit_comments.json'
GERRIT_COMMENTS_DIR = os.path.join(ARTIFACTS_DIR, 'gerrit_comments')

# Where we drop the test results for ingestion into sponge.  Since it only
# handles XML junit/xunit results, we need to format+output manually.
# Kokoro can handle more than one file, but they must all be named the same
# "sponge_log.log" & "sponge_log.xml" (we'll ignore retry support as we don't
# use it).  So we have to namespace the files under dirs.
# We'll use a convention like sponge/$PKG/$TOOL/.
# NB: sponge_log.log is only loaded if sponge_log.xml is found.
SPONGE_LOG_NAME = 'sponge_log.log'
SPONGE_XML_NAME = 'sponge_log.xml'
SPONGE_RESULTS_DIR = os.path.join(ARTIFACTS_DIR, 'sponge')


def is_presubmit():
    """Whether we're testing a pending CL."""
    # This env var is passed down by kokoro.
    return bool(os.environ.get('KOKORO_GERRIT_REVISION'))


def should_build_plugin():
    """Whether we should build the ssh_client plugin code.

    Building this native code with the nacl toolchain dominates our build times
    (usually ~75% of the entire build).  Lets skip it if we don't think any of
    changes are relevant.
    """
    logging.info('Detecting whether to build plugin')

    # Release builders always build the plugin.
    if not is_presubmit():
        logging.info('Release builder: always building the plugin!')
        return True

    check_paths = (
        # The CI itself.
        'kokoro/*',
        # Utility libraries.
        'libdot/bin/*',
        # The plugin code itself.
        'ssh_client/*',
    )
    result = libdot.run(['git', 'show', '--format=', '--name-only',
                         os.environ['KOKORO_GERRIT_REVISION']],
                        capture_output=True, cwd=kokoro.LIBAPPS_DIR)
    relevant_paths = list(
        x for x in result.stdout.decode('utf-8').splitlines()
        if any(fnmatch.fnmatch(x, check) for check in check_paths))
    if relevant_paths:
        logging.info('Presubmit: some files might affect the plugin:\n%s',
                     '\n'.join(relevant_paths))
    else:
        logging.info('Presubmit: no files affect the plugin; skipping')
    return bool(relevant_paths)


def build_archive():
    """Move compiled files to artifacts output tree."""
    # Unpack the nassh extensions so they can be packaged & signed.
    nassh_dir = os.path.join(kokoro.LIBAPPS_DIR, 'nassh')
    dist_dir = os.path.join(nassh_dir, 'dist')
    for archive in glob.glob(os.path.join(dist_dir, '*.zip')):
        # The archives look like:
        #   SecureShellApp-0.20.zip
        #   SecureShellApp-dev-0.20.7220.2157.zip
        # We want to output dirs like:
        #   SecureShellApp/
        #   SecureShellApp-dev/
        output = os.path.basename(archive).rsplit('-', 1)[0]
        libdot.run(['unzip', archive, '-d',
                    os.path.join(ARTIFACTS_DIR, output)])

    # Copy over the nacl ssh build with debug info.
    if should_build_plugin():
        ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, 'ssh_client')
        os.rename(os.path.join(ssh_client_dir, 'output', 'release.tar.xz'),
                  os.path.join(ARTIFACTS_DIR, 'ssh_client-release.tar.xz'))


def build():
    """Build the various components."""
    if should_build_plugin():
        # Build the ssh client code.
        ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, 'ssh_client')
        cmd = ['./build.sh']
        if not is_presubmit():
            cmd += ['--official-release']
        libdot.run(cmd, cwd=ssh_client_dir)
        # Copy output files.
        libdot.run(['cp', '-r', './ssh_client/output/plugin/', './nassh/'],
                   cwd=kokoro.LIBAPPS_DIR)

    nassh_dir = os.path.join(kokoro.LIBAPPS_DIR, 'nassh')

    # If updated translations exist, use them.
    tot_l10n = os.path.join(INPUT_ARTIFACTS_DIR, 'translations')
    if os.path.exists(tot_l10n):
        libdot.run(['./bin/import-translations', '--skip-git',
                    os.path.join(tot_l10n, 'messages_fs', '_locales')],
                   cwd=nassh_dir)

    # Build the nassh program.
    dist_dir = os.path.join(nassh_dir, 'dist')
    shutil.rmtree(dist_dir, ignore_errors=True)
    libdot.run(['./bin/mkdist'], cwd=nassh_dir)

    build_archive()


def _run_linter(cmd, cwd):
    """Run the linter taking into account presubmit bot state."""
    result = libdot.run(cmd, check=False, cwd=cwd)
    if result.returncode:
        msg = f'{cmd[0]} linter exited {result.returncode}'
        if is_presubmit():
            logging.error(msg)
            return False
        else:
            logging.warning('%s; ignoring for release builds', msg)
    return True


def test_lint():
    """Lint all our files."""
    # Log versions of linter tools since we pull some from the container.
    libdot.run([kokoro.DIR / 'lint', '--version'])

    ret = []
    for pkg in ('kokoro', 'libdot', 'hterm', 'nassh', 'ssh_client', 'terminal',
                'wasi-js-bindings', 'wassh'):
        pkg_dir = kokoro.LIBAPPS_DIR / pkg
        linter = pkg_dir / 'lint'
        if not linter.exists():
            linter = pkg_dir / 'bin' / 'lint'
        cmd = [
            linter,
            '--gerrit-comments-file',
            os.path.join(GERRIT_COMMENTS_DIR, pkg, '%(tool)',
                         GERRIT_COMMENTS_NAME),
        ]
        if not _run_linter(cmd, pkg_dir):
            ret.append(('lint', pkg))
    return ret


def test_unittests():
    """Run all the unittests."""
    ret = []
    test_args = ['--no-sandbox', '--reporter', 'xunit']
    for pkg in ('libdot', 'hterm', 'nassh', 'terminal', 'wasi-js-bindings'):
        pkg_dir = os.path.join(kokoro.LIBAPPS_DIR, pkg)
        if os.path.exists(os.path.join(pkg_dir, 'package.json')):
            cmd = ['test', '--'] + test_args
            run = libdot.npm.run
        else:
            cmd = ['./bin/load_tests'] + test_args
            run = libdot.run
        result = run(cmd, check=False, cwd=pkg_dir,
                     stdout=subprocess.PIPE, stderr=subprocess.STDOUT)

        if result.returncode:
            logging.error('%s test suite exited %i', pkg, result.returncode)
            ret.append(('unittests', pkg))

        # Dump the results to log files for sponge to ingest.
        sponge_dir = os.path.join(SPONGE_RESULTS_DIR, pkg, 'unittests')
        os.makedirs(sponge_dir)
        log_file = os.path.join(sponge_dir, SPONGE_LOG_NAME)
        with open(log_file, 'wb') as fp:
            fp.write(result.stdout)

        # The test helper will output stuff before/after the XML file, so we
        # need to scrape it out by hand.
        m = re.search(br'^(.*)(<testsuite[ >].*</testsuite>)(.*)$',
                      result.stdout, flags=re.DOTALL)
        if m:
            log_header, xml_data, log_footer = m.groups()

            xml_file = os.path.join(sponge_dir, SPONGE_XML_NAME)
            with open(xml_file, 'wb') as fp:
                fp.write(xml_data)

            print(log_header.decode('utf-8'))
            print(f'<xml data is in {xml_file}>')
            print(log_footer.decode('utf-8'))
        else:
            logging.error('Unable to parse xml results from log output!\n%s',
                          result.stdout.decode('utf-8'))
            ret.append(('unittests', pkg))

    return ret


def test():
    """Test the various components."""
    # Run all the tests before aborting.
    failures = test_lint() + test_unittests()
    if failures:
        logging.error('Some tests failed!  See above for details.')
        for (step, pkg) in failures:
            logging.error('  %s failed %s phase.', pkg, step)
        sys.exit(1)


def clean():
    """Clean up various compiled objects.

    When kokoro is done (pass or fail), it archives the tree via rsync.
    Remove all the compiled objects since we don't need them.
    """
    ssh_client_dir = os.path.join(kokoro.LIBAPPS_DIR, 'ssh_client')
    shutil.rmtree(os.path.join(ssh_client_dir, 'output'), ignore_errors=True)

    shutil.rmtree(libdot.node.NODE_MODULES_DIR, ignore_errors=True)

    # Sync uid/gid settings in the output artifacts.  The docker container
    # runs all code as root, but the accounts outside won't be that.  Look
    # at the source tree created outside the container for the right values.
    st = os.stat(kokoro.LIBAPPS_DIR)
    uid = st.st_uid
    gid = st.st_gid
    if uid != os.getuid():
        def chown(path):
            """Helper to chown |path| to |uid| & |gid|."""
            if os.path.islink(path):
                return
            try:
                os.chown(path, uid, gid)
            except OSError as e:
                logging.warning('Unable to chown(%s, %s, %s): %s',
                                path, uid, gid, e)

        logging.info('Changing ownership of artifacts dir to %i:%i', uid, gid)
        chown(ARTIFACTS_DIR)
        for root, dirs, files in os.walk(ARTIFACTS_DIR):
            for d in dirs:
                chown(os.path.join(root, d))
            for f in files:
                chown(os.path.join(root, f))


def setup():
    """Set up the overall state before we build/test."""
    # Log some system state so we can see current CI/container state.
    libdot.run(['uname', '-a'])
    libdot.run(['cat', '/etc/os-release'], check=False)
    libdot.run(['env'])

    # Initialize the artifacts output.
    logging.info('Setting up artifacts output %s', ARTIFACTS_DIR)
    shutil.rmtree(ARTIFACTS_DIR, ignore_errors=True)
    os.makedirs(ARTIFACTS_DIR)

    # Create the dropbox for gerrit comments.
    os.makedirs(GERRIT_COMMENTS_DIR)
    # Always create a stub comments file.  The kokoro docs don't say this is
    # required, but other users report it doesn't post updates back to Gerrit
    # unless it exists, and we're seeing that same behavior.
    # http://g/kokoro-users/coBW2tNu46g/W8PozUUpAQAJ
    stub = os.path.join(GERRIT_COMMENTS_DIR, GERRIT_COMMENTS_NAME)
    with open(stub, 'wb') as fp:
        fp.write(b'[]')


def get_parser():
    """Get a command line parser."""
    parser = libdot.ArgumentParser(description=__doc__)
    parser.add_argument('--skip-clean', dest='clean',
                        action='store_false', default=True,
                        help='Clean up compiled object dirs.')
    return parser


def main(argv):
    """The main func!"""
    parser = get_parser()
    opts = parser.parse_args(argv)
    libdot.node_and_npm_setup()

    setup()
    try:
        build()
        test()
        logging.info('All builds & tests passed!')
    finally:
        if opts.clean:
            clean()

if __name__ == '__main__':
    sys.exit(main(sys.argv[1:]))
